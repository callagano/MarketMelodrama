name: Economic Calendar Scraper

on:
  schedule:
    # Run daily at 7 AM UTC
    - cron: '0 7 * * *'
  workflow_dispatch:  # Allow manual triggering
  push:
    paths:
      - 'src/scripts/**'
      - '.github/workflows/economic-calendar-scraper.yml'

jobs:
  scrape-economic-calendar:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        cd src/scripts
        pip install -r requirements_scraper.txt
        
    - name: Create data directory
      run: |
        mkdir -p src/scripts/economic_data
        
    - name: Run economic calendar scraper
      run: |
        cd src/scripts
        python3 scheduled_scraper.py
      env:
        PYTHONPATH: ${{ github.workspace }}/src/scripts
        
    - name: Upload scraped data as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: economic-calendar-data-${{ github.run_number }}
        path: |
          src/scripts/economic_data/
          src/scripts/*.log
        retention-days: 30
        
    - name: Commit and push data files
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add scraped data files
        git add src/scripts/economic_data/
        git add src/scripts/*.log || true
        
        # Commit if there are changes
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "ðŸ¤– Auto-update economic calendar data [skip ci]"
          git push
        fi 